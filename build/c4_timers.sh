#!/bin/bash
cd ${TWB_BASE}/config/jobs/twb_c4
kinit svc-ra-bicoeloaddev@RA-INT.COM -k -t svc-ra-bicoeloaddev.keytab
export HADOOP_CONF_DIR=/etc/hadoop/conf:/etc/hive/conf
nohup spark-submit --master yarn-cluster --name c4_timers --keytab "svc-ra-bicoeloaddev.keytab" --principal "svc-ra-bicoeloaddev@RA-INT.COM" --conf "spark.dynamicAllocation.enabled=false" --num-executors 3 --driver-memory 3g --executor-memory 3g --conf "spark.streaming.unpersist=true" --conf "spark.streaming.kafka.maxRatePerPartition=15"  --conf "spark.executor.heartbeatInterval=20s" --conf "spark.streaming.kafka.maxRetries=5" --conf "spark.streaming.backpressure.enabled=true" --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC -Djava.security.krb5.conf=/etc/krb5.conf" --conf "spark.driver.extraJavaOptions=-XX:+UseG1GC -Djava.security.krb5.conf=/etc/krb5.conf" --conf spark.yarn.stagingDir=hdfs:///tmp/spark/ --files "c4_timers.properties" --conf spark.yarn.maxAppAttempts=4 --conf spark.yarn.max.executor.failures=24 --conf spark.task.maxFailures=8 --conf spark.hadoop.fs.hdfs.impl.disable.cache=true ${TWB_BASE}/build/powerbi-streaming2-0.0.1-SNAPSHOT.jar c4_timers.properties > ${TWB_BASE}/loadJobs/logs/twb_c4/c4_timers.log 2>&1 & echo $! > c4_ftdefects.pid